# PRINCIPIOS DE QUÍMICA
- ## Definiciones de entropía
- 1 Definición historica
      
    ---
- 2 Definición bajo la termodinámica estadística:

  Para entender la entropía bajo este paradigma primero debemos definir que son los micro y macro estados.
  
  Definamos a modo de ilustración un sistema, un mol de gas confinado cuya concentración, presión y temperatura son constantes, al observar recordamos que cada una de las partículas confinadas está en movimiento, por lo que en cada instante todas y cada una de ellas van a tener una posición y energía cinética partículas.

  El estado en el cual las propiedades macroscópicas han sido definidas (nuestro gas confinado) es un macro estado, los microestados van a ser todas las posibilidades dentro de ese macro estado, es decir todas las posibles combinaciones de energía cinética y ubicación de cada una de las partículas dentro del macro estado.

  Por consiguiente cada macro estado o estado termodinámico contiene dentro de si un número de microestados posibles (W) .

  $$
S = k\ ln W
$$

<div align="center">
    
| Símbolo | Significado |
|----------|-----------|
| S  | Entropía    | 
| k   | Constante de Bolztmann   |
| W  | Numero de microestados posibles    |

</div>

  Por lo que vemos la entropía como la medida de la cantidad de microestados posibles dentro de un estado macroscópico determinado.[^1][^2]

  ---

- 3 Definición bajo la gestión de información:

    La entropía, en el contexto de la gestión de la información, es una medida matemática que describe el estado de desorganización o desorden de un sistema. Este concepto se define por una relación inversa con la información estructural $I_s$, partiendo de la premisa de que toda estructura organizada contiene información. En consecuencia, un aumento de entropía se traduce en una pérdida de organización y, por lo tanto, una pérdida de información estructural. Como indicador de la calidad de la información dentro de un repositorio, la baja entropía es deseable, pues refleja un alto grado de organización y estructura del sistema, mientras que los valores elevados de entropía indican un mayor desorden.
  

$$
H = -K \sum_{i=1}^{n} p_i \ln(p_i)
$$

 <div align="center">
    
| Símbolo | Significado |
|----------|-----------|
| H  | Entropía del sistema    | 
| k   | Constante de proporcionalidad   |
| $p_i$  | Probabilidad del evento i    |

</div>

 [^3][^4]
 
- 4

 
- 5
      
  


---
## Bibliografía
[^1]: Çengel, Y. A., & Boles, M. A. (with Martínez Bautista, A. L.). (2015). Termodinámica. McGraw-Hill.
[^2]: Brown, T. L., LeMay, H. E., Bursten, B. E., Murphy, C. J., & Woodward, P. M. (with García Hernández, A. E.). (2014). Química: La ciencia central (12a ed.). Pearson Educación.
[^3]: Tarragó, J. C. P., Ávila, R. M. Á., Gallardo, M. D. C. E., & Gálvez, D. L. D. (2020). La gestión de la información en un enfoque a partir de la entropía. Revista Científica Sinapsis, 1(16).
[^4]: Stonier, T. (1989). Towards a general theory of information II: Information and entropy. Aslib Proceedings, 41(2), 41-55. https://doi.org/10.1108/eb051124

